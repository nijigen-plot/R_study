library(dplyr)
install(glmnet)
install.packages(glmnet)
install.packages('glmnet')
install.pacakges('MASS')
insta..packages('MASS')
install.packages('MASS')
library(mvtnorm)
library(MASS)
n_train <- 100
n_test <- 1000
p <- 5
beta_0 <- 1 ## 定数項
beta <- c(1, 0, -1, 0, 0)
beta <- c(beta_0, beta)
beta
mu <- numeric(p)
mu
diag(p)
Σ = diag(p)
σ <- 1
X <- rmvnorm((n_train+n_test), mu, Σ)
X
View(X)
cbind(rep(1, (n_train+n_test)),X)
X <- cbind(rep(1, (n_train+n_test)),X)
summarise(X)
summarise(X)
X.maen()
X.mean()
mean(X)
ε <- rnorm((n_train+mtest),0,σ)
ε <- rnorm((n_train+n_test),0,σ)
X%*%beta
y <- X%*%beta+ε
View(y)
View(X)
view(beta)
view(beta)
library(tidyverse)
view(beta)
View(X)
# それぞれ見ればわかるけど、ベクトルの形で回帰になってる
cbind(X, y)
data <- cbind(X, y)
as.data.frame(data)
data <- as.data.frame(data)
View(data)
colnames(data) <- c('int','x1', 'x2','x3','x4','x5','y')
data[1:n_train,]
data[1:4]
data_train <- data[1:n_train,] ## 点が重要よ
data[,3:5]
data_test <- data[-(1:n_train),]
data_test
View(data_train)
View(data_test)
View(data_train)
View(data_test)
1:n_train
-(1:n_train)
# glm(普通の線形回帰)
data_train[,-1]
result_glm <- glm(formula= y ~ x1 + x2 + x3 + x4 + x5, data=data_train[,-1], family=gaussian(link='identity'))
result_glm_1 <- glm(formula= y ~ x1 + x2 + x3 + x4 + x5, data=data_train, family=gaussian(link='identity'))
result_glm
result_glm_1
as.matrix(result_glm$coefficients)
#glmnet(lasso)
result_lasso <- cv.glmnet(x=as.matrix(data_train[,c('x1','x2','x3','x4','x5')]), y=data_train$y, family="gaussian", alpha=1)
library(glmnet)
result_lasso <- cv.glmnet(x=as.matrix(data_train[,c('x1','x2','x3','x4','x5')]), y=data_train$y, family="gaussian", alpha=1)
result_lasso
coefficients(result_lasso, x='lambda.min')
result_ridge <- cv.glmnet(x=as.matrix(data_train[, c('x1','x2','x3','x4','x5')]), y=data_train$y, family='gaussian', alpha=0)
coefficients(result_ridge, s='lambda.min')
as.matrix(data_test[,colunames(data_test)!='y'])
as.matrix(data_test[,colnames(data_test)!='y'])
View(data_test)
result_glm$coefficients
as.matrix(result_glm$coefficients)
yhat_glm <- as.matrix(data_test[, colnames(data_test)!='y']) %*% as.matrix(result_glm$coefficients)
yhat_glm
mean((data_test$y - yhat_glm)^2)
glm_mse = mean((data_test$y - yhat_glm)^2)
yhat_lasso <- as.matrix(data_test[, colnames(data_test)!= 'y']) %*% coefficients(result_lasso, s='lambda.min')
lasso_mse <- mean((data_test$y - yhat_lasso)^2)
yhat_ridge <- as.matrix(data_test[, colnames(data_test)!= 'y']) %*% coefficients(result_ridge, s='lambda.min')
ridge_mse <- mean((data_test$y - yhat_ridge)^2)
print(glm_mse, lasso_mse, ridge_mse)
glm_mse
lasso_mse
ridge_mse
result_glm
result_glm$coefficients
sum(as.matrix(result_glm$coefficients)^2)
sum(coefficients(result_lasso, s='lambda.min')^2)
sum(coefficients(result_ridge, s='lambda.min')^2)
coefficients(result_ridge, s='lambda.min')^2
coefficients(result_ridge, s='lambda.min')
glm_mse
lasso_mse
ridge_mse
# ランダムな値を１回だして、評価しただけでは精度評価できないので、繰り返す
# RMEで、実測と予測を比較することを、RMPEというってさ
times <- 100 #シミュレーション回数
p <- 50 #説明変数の数（次元数）は５０以下でランダムに変動する
beta <- c(1, -1, numeric(p-2))
beta
beta <- c(beta_0, beta)
beta
mu <- numeric(p)
mu
Σ <- diag(p)
Σ
MSPE <- data.frame(glm=numeric(times)+NaN,
lasso=numeric(times)+NaN,
ridge=numeric(times)NaN)
MSPE <- data.frame(glm=numeric(times)+NaN, lasso=numeric(times)+NaN, ridge=numeric(times)NaN)
numeric(times)
numeric(times)+NaN
MSPE <- data.frame(glm=numeric(times)+NaN)
MSPE <- data.frame(glm=numeric(times)+NaN, lasso=numeric(times)+NaN, ridge=numeric(times)NaN)
MSPE <- data.frame(glm=numeric(times)+NaN, lasso=numeric(times)+NaN, ridge=numeric(times)+NaN)
MSPE
# MSPEは評価指標結果格納テーブル化
}
for(t in 1:times){
X <- rmvnorm((n_train+n_test),mu,Σ)
X <- cbind(rep(1,(n_train+n_test)),X)
ε <- rnorm((n_train+n_test),0,σ)
y <- X%*%beta+ε
data <- cbind(X, y)
data <- as.data.frame(data)
colnames(data) <- c('int', paste0('x', 1:p), 'y')
data_train <- data[1:n_train,]
data_test <- data[-(1:n_train),]
result_glm <- glm(formula= y ~., data=data_train[, colnames(data_train)!= 'int'], family=gaussian(link='identity'))
result_lasso <- cv.glmnet(x=as.matrix(data_train[, !(colnames(data_train) %in% c('int','y'))]),
y=data_train$y, family='gaussian', alpha=1)
result_ridge <- cv.glmnet(x=as.matrix(data_train[, !(colnames(data_train) %in% c('int','y'))]),
y=data_train$y, family='gaussian', alpha=0)
yhat_glm <- as.matrix(data_test[, colnames(data_test)!= 'y']) %*% as.matrix(result_glm$coefficients)
MSPE$glm[t] <- mean((data_test$y - yhat_glm)^2)
yhat_lasso <- as.matrix(data_test[, colnames(data_test)!= 'y'])%*% coefficients(result_lasso, s='lambda.min')
MSPE$lasso[t] <- mean((data_test$y - yhat_lasso)^2)
yhat_ridge <- as.matrix(data_test[, colnames(data_test)!= 'y'])%*% coefficients(result_ridge, s='lambda.min')
MSPE$ridge[t] <- mean((data_test$y - yhat_ridge)^2)
cat('t=',t,' ')
}
View(MSPE)
g <- ggplot(gather(MSPE), aes(y=value, x=key))
g <- g + geom_boxplot()
g <- g + xlab('Method')
g <- g + ylab('MSPE')
plot(g)
